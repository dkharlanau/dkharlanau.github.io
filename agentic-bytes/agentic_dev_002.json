{
  "byte_id": "agentic_dev_002",
  "title": "Tools vs Chat: When an Agent Must Act, Not Just Talk",
  "level": "foundation",
  "domain": [
    "agentic-development",
    "tool-calling",
    "reliability"
  ],
  "intent": "Learn to clearly distinguish between 'thinking in text' and 'acting on the world', and explain why serious agents must use tools.",
  "core_idea": {
    "one_liner": "Chat is for reasoning and explanation; tools are for facts, actions, and truth.",
    "why_it_matters": [
      "Without tools, an agent can only guess.",
      "Tools turn an LLM from a storyteller into a worker.",
      "Most hallucinations come from using chat where tools were required."
    ]
  },
  "definitions": {
    "chat": "Pure language generation used for reasoning, summarizing, explaining, or drafting.",
    "tool": "An external capability that reads or changes real state (API, DB, file system, search, calculator)."
  },
  "decision_rule": {
    "golden_rule": "If the answer depends on real data or causes side effects, the agent must use a tool.",
    "quick_test": [
      "Would a human need to check a system to answer this?",
      "Could being wrong cause damage or confusion?",
      "Does the result need to be reproducible?"
    ]
  },
  "when_chat_is_enough": [
    "Explaining concepts or theory",
    "Summarizing already-provided text",
    "Drafting emails or documents (with no new facts)",
    "Brainstorming alternatives"
  ],
  "when_tools_are_required": [
    "Looking up current or system-specific data",
    "Checking status, logs, queues, metrics",
    "Creating or modifying records",
    "Validating assumptions",
    "Producing structured outputs based on real inputs"
  ],
  "common_tools": [
    {
      "tool_type": "Retrieval (RAG/Search)",
      "used_for": "Facts, procedures, policies, past decisions",
      "risk_if_not_used": "Outdated or invented information"
    },
    {
      "tool_type": "System APIs / DB",
      "used_for": "State inspection and updates",
      "risk_if_not_used": "Wrong diagnosis or fake confidence"
    },
    {
      "tool_type": "Calculator / Code",
      "used_for": "Metrics, formulas, transformations",
      "risk_if_not_used": "Math errors hidden in prose"
    }
  ],
  "agent_behavior_pattern": {
    "name": "Think → Decide → Act",
    "description": "The agent reasons in chat, then explicitly decides whether a tool call is required before answering.",
    "rule": "No final answer before all required tool calls are completed or explicitly refused with justification."
  },
  "micro_example": {
    "scenario": "User asks: 'Is BP replication currently delayed?'",
    "bad_agent": "Answers from memory or assumptions.",
    "good_agent": {
      "decision": "This depends on live system state → tool required.",
      "action": "Call monitoring API / check queue metrics.",
      "answer": "Reports status with evidence or says data is unavailable."
    }
  },
  "failure_modes": [
    "Tool avoidance (agent prefers to talk instead of act)",
    "Fake tool usage (claims it checked, but didn’t)",
    "Overusing tools for simple reasoning (slow and expensive)"
  ],
  "guards": [
    "If a question requires tools and tools are unavailable, the agent must say 'I cannot verify this right now.'",
    "Never invent tool outputs.",
    "Log every tool call with inputs and outputs."
  ],
  "teach_it_in_english": {
    "simple_explanation": "Chat is the agent thinking out loud. Tools are how it touches reality. If reality matters, tools are mandatory.",
    "one_sentence_definition": "A reliable agent knows when words are enough and when actions are required."
  },
  "practical_checklist": [
    "Does this answer depend on real data?",
    "Could a wrong answer cause harm?",
    "Did the agent clearly decide to use or not use a tool?",
    "Are tool results referenced explicitly?"
  ],
  "tags": [
    "tool-calling",
    "hallucination-prevention",
    "decision-rule",
    "agent-design"
  ]
}